{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/anna/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/anna/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anna/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/anna/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anna/anaconda3/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/anna/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/anna/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No news section found for ASX code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ASX symbols fetched: 2073\n",
      "First 10 ASX symbols: ['ASX code', '14D', '29M', 'T3D', 'TGP', 'TCF', 'TOT', 'TDO', '3PL', '4DX']\n",
      "\n",
      "Fetching news for ASX code\n",
      "No news found for ASX code\n",
      "\n",
      "Fetching news for 14D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No news section found for 14D\n",
      "ERROR:root:No news section found for 29M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news found for 14D\n",
      "\n",
      "Fetching news for 29M\n",
      "No news found for 29M\n",
      "\n",
      "Fetching news for T3D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No news section found for T3D\n",
      "ERROR:root:No news section found for TGP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news found for T3D\n",
      "\n",
      "Fetching news for TGP\n",
      "No news found for TGP\n",
      "\n",
      "Fetching news for TCF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No news section found for TCF\n",
      "ERROR:root:No news section found for TOT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news found for TCF\n",
      "\n",
      "Fetching news for TOT\n",
      "No news found for TOT\n",
      "\n",
      "Fetching news for TDO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No news section found for TDO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news found for TDO\n",
      "\n",
      "Fetching news for 3PL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No news section found for 3PL\n",
      "ERROR:root:No news section found for 4DX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No news found for 3PL\n",
      "\n",
      "Fetching news for 4DX\n",
      "No news found for 4DX\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# Function to fetch all ASX symbols from the ASX CSV file\n",
    "def get_all_asx_symbols():\n",
    "    url = \"https://www.asx.com.au/asx/research/ASXListedCompanies.csv\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Use StringIO to read the CSV file\n",
    "        csv_data = StringIO(response.text)\n",
    "        csv_reader = csv.reader(csv_data)\n",
    "\n",
    "        symbols = []\n",
    "        next(csv_reader)  # Skip the header row\n",
    "\n",
    "        # Process each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            if len(row) > 1:  # Ensure there are at least 2 columns\n",
    "                asx_code = row[1].strip()  # The ASX code is in the second column\n",
    "                if asx_code:  # Ensure it's not an empty string\n",
    "                    symbols.append(asx_code)\n",
    "        \n",
    "        return symbols\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error fetching ASX symbols: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to get the latest news for an ASX stock\n",
    "def get_asx_latest_news(stock_code):\n",
    "    url = f\"https://www.asx.com.au/asx/share-price-research/company/{stock_code}\"\n",
    "\n",
    "    try:\n",
    "        # Make an HTTP GET request to fetch the page content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check if request was successful\n",
    "        \n",
    "        # Parse the page with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the section of the webpage that contains the news articles\n",
    "        news_section = soup.find('div', class_='company-news')  # Update the correct class based on the webpage structure\n",
    "\n",
    "        if not news_section:\n",
    "            logging.error(f\"No news section found for {stock_code}\")\n",
    "            return []\n",
    "\n",
    "        # Extract each news article\n",
    "        news_items = news_section.find_all('div', class_='news-item')  # Adjust based on the webpage structure\n",
    "        news_data = []\n",
    "\n",
    "        for item in news_items:\n",
    "            # Extract the news title\n",
    "            title_tag = item.find('a', class_='news-title')  # Update the class or tag based on the webpage\n",
    "            title = title_tag.text.strip() if title_tag else \"No title\"\n",
    "\n",
    "            # Extract the news link\n",
    "            link = title_tag['href'] if title_tag else \"#\"\n",
    "\n",
    "            # Extract the publication date\n",
    "            date_tag = item.find('span', class_='news-date')  # Update the class or tag based on the webpage\n",
    "            date = date_tag.text.strip() if date_tag else \"No date\"\n",
    "\n",
    "            # Append the extracted data\n",
    "            news_data.append({\n",
    "                'title': title,\n",
    "                'link': f\"https://www.asx.com.au{link}\",\n",
    "                'date': date\n",
    "            })\n",
    "\n",
    "        return news_data\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error fetching news for {stock_code}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to fetch and print news content from the article page\n",
    "def fetch_news_content(news_url):\n",
    "    try:\n",
    "        # Fetch the news article content\n",
    "        response = requests.get(news_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the main content from the article page\n",
    "        # This will depend on the structure of the news article page, adjust accordingly\n",
    "        content_section = soup.find('div', class_='article-content')  # Adjust class name based on real HTML structure\n",
    "        if content_section:\n",
    "            return content_section.get_text(strip=True)\n",
    "        else:\n",
    "            return \"No content available.\"\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error fetching content from {news_url}: {e}\")\n",
    "        return \"Error fetching content.\"\n",
    "\n",
    "# Function to save news content to a file named {symbol}_news.txt\n",
    "def save_news_to_file(symbol, news_data):\n",
    "    filename = f\"{symbol}_news.txt\"\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for news in news_data:\n",
    "            f.write(f\"Title: {news['title']}\\n\")\n",
    "            f.write(f\"Date: {news['date']}\\n\")\n",
    "            f.write(f\"Link: {news['link']}\\n\")\n",
    "\n",
    "            # Fetch and write the news content\n",
    "            content = fetch_news_content(news['link'])\n",
    "            f.write(f\"Content: {content}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    print(f\"Saved news for {symbol} in {filename}\")\n",
    "\n",
    "# Main program\n",
    "def main():\n",
    "    # Fetch all ASX symbols\n",
    "    symbols = get_all_asx_symbols()\n",
    "\n",
    "    # Check if any symbols were fetched\n",
    "    if not symbols:\n",
    "        print(\"No symbols found.\")\n",
    "        return\n",
    "\n",
    "    # Limit to the first 10 symbols for demo purposes\n",
    "    print(f\"Total number of ASX symbols fetched: {len(symbols)}\")\n",
    "    print(f\"First 10 ASX symbols: {symbols[:10]}\")\n",
    "    \n",
    "    for symbol in symbols[:10]:  # Limit it to 10 symbols for this demo\n",
    "        print(f\"\\nFetching news for {symbol}\")\n",
    "        latest_news = get_asx_latest_news(symbol)\n",
    "\n",
    "        # Save the latest news for each stock\n",
    "        if latest_news:\n",
    "            save_news_to_file(symbol, latest_news)\n",
    "        else:\n",
    "            print(f\"No news found for {symbol}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
